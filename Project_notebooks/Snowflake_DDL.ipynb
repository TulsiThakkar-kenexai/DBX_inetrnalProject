{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89303252-89ad-4929-a915-568fcb50a99c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "pipeline-1 with JDBC connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b33dd4fe-c83a-4124-9ee5-c81d901fc6bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/', name='Bronze_layer/', size=0, modificationTime=1749200643000),\n",
       " FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/gold_layer/', name='gold_layer/', size=0, modificationTime=1749200665000),\n",
       " FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/sliver_layer/', name='sliver_layer/', size=0, modificationTime=1749200652000)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Databricks intenal Project\n",
    "\n",
    "#External Table loading in Bronze_layer\n",
    "\n",
    "#Service principal configration\n",
    "\n",
    "client_id = \"06c11865-4034-4363-9e84-720eb4487b8b\"\n",
    "tenant_id = \"1c292f40-07c6-4a6a-9106-a006f98c2da1\"\n",
    "client_secret = \"kWN8Q~r3BTjP3DaJ-3VHoDZInJpI5VJ.pNAFkaxm\"\n",
    "storage_account_name = \"dbxinternalproject\"\n",
    "container_name = \"dbxintenalproccontainer\"\n",
    "\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret )\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n",
    "\n",
    "dbutils.fs.ls(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df9a80a-9e99-46d8-9779-197d3413cbef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snowflake-connector-python\n  Obtaining dependency information for snowflake-connector-python from https://files.pythonhosted.org/packages/30/5e/3b325c21e91df06b1868c791f384efd44b142c6d777f9c9322148ccb0a05/snowflake_connector_python-3.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached snowflake_connector_python-3.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (70 kB)\nCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n  Obtaining dependency information for asn1crypto<2.0.0,>0.24.0 from https://files.pythonhosted.org/packages/c9/7f/09065fd9e27da0eda08b4d6897f1c13535066174cc023af248fc2a8d5e5a/asn1crypto-1.5.1-py2.py3-none-any.whl.metadata\n  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: boto3>=1.24 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (1.34.39)\nRequirement already satisfied: botocore>=1.24 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (1.34.39)\nRequirement already satisfied: cffi<2.0.0,>=1.9 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (1.15.1)\nRequirement already satisfied: cryptography>=3.1.0 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (41.0.3)\nCollecting pyOpenSSL<26.0.0,>=22.0.0 (from snowflake-connector-python)\n  Obtaining dependency information for pyOpenSSL<26.0.0,>=22.0.0 from https://files.pythonhosted.org/packages/80/28/2659c02301b9500751f8d42f9a6632e1508aa5120de5e43042b8b30f8d5d/pyopenssl-25.1.0-py3-none-any.whl.metadata\n  Using cached pyopenssl-25.1.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: pyjwt<3.0.0 in /usr/lib/python3/dist-packages (from snowflake-connector-python) (2.3.0)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (2022.7)\nRequirement already satisfied: requests<3.0.0 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (2.31.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (23.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (2023.7.22)\nRequirement already satisfied: typing_extensions<5,>=4.3 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (4.10.0)\nRequirement already satisfied: filelock<4,>=3.5 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (3.13.4)\nCollecting sortedcontainers>=2.4.0 (from snowflake-connector-python)\n  Obtaining dependency information for sortedcontainers>=2.4.0 from https://files.pythonhosted.org/packages/32/46/9cb0e58b2deb7f82b84065f37f3bffeb12413f947f9388e4cac22c4621ce/sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata\n  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /databricks/python3/lib/python3.11/site-packages (from snowflake-connector-python) (3.10.0)\nCollecting tomlkit (from snowflake-connector-python)\n  Obtaining dependency information for tomlkit from https://files.pythonhosted.org/packages/bd/75/8539d011f6be8e29f339c42e633aae3cb73bffa95dd0f9adec09b9c58e85/tomlkit-0.13.3-py3-none-any.whl.metadata\n  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (0.10.0)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.11/site-packages (from boto3>=1.24->snowflake-connector-python) (0.10.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (2.8.2)\nRequirement already satisfied: urllib3<2.1,>=1.25.4 in /databricks/python3/lib/python3.11/site-packages (from botocore>=1.24->snowflake-connector-python) (1.26.16)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.21)\nCollecting cryptography>=3.1.0 (from snowflake-connector-python)\n  Obtaining dependency information for cryptography>=3.1.0 from https://files.pythonhosted.org/packages/e7/53/8a130e22c1e432b3c14896ec5eb7ac01fb53c6737e1d705df7e0efb647c6/cryptography-45.0.3-cp311-abi3-manylinux_2_34_x86_64.whl.metadata\n  Using cached cryptography-45.0.3-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.24->snowflake-connector-python) (1.16.0)\nUsing cached snowflake_connector_python-3.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\nUsing cached asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\nUsing cached pyopenssl-25.1.0-py3-none-any.whl (56 kB)\nUsing cached cryptography-45.0.3-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\nUsing cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\nUsing cached tomlkit-0.13.3-py3-none-any.whl (38 kB)\nInstalling collected packages: sortedcontainers, asn1crypto, tomlkit, cryptography, pyOpenSSL, snowflake-connector-python\n  Attempting uninstall: cryptography\n    Found existing installation: cryptography 41.0.3\n    Not uninstalling cryptography at /databricks/python3/lib/python3.11/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dc27bc79-940a-4fac-8e9d-0986d3b3247d\n    Can't uninstall 'cryptography'. No files were found to uninstall.\nSuccessfully installed asn1crypto-1.5.1 cryptography-45.0.3 pyOpenSSL-25.1.0 snowflake-connector-python-3.15.0 sortedcontainers-2.4.0 tomlkit-0.13.3\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install snowflake-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6bd9861-4f72-4c50-bc15-31e62fa6cf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta Tables found: ['Average_Delivery_Time_by_Shipping_Method', 'Average_Product_Rating', 'Brand_Loyalty_Score', 'Brand_Return_Rate', 'Customer_Retention_Rate', 'Monthly_Churn_Rate', 'Most_Product_Purchase_Frequency', 'Payment_Method_Preference', 'Repeat_Purchase_Score_by_Product', 'customers_purchase_same_product', 'daywise_trends', 'monthwise_trends', 'product_purchase_by_Revenue_and_gender', 'revenue_risk_returns', 'weekdaywise_trends', 'yearwise_trends']\n"
     ]
    }
   ],
   "source": [
    "import snowflake.connector\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# table_list = [\n",
    "#     \"Customer_Retention_Rate\",\n",
    "#     \"Monthly_Churn_Rate\",\n",
    "#     \"Brand_Return_Rate\",\n",
    "#     \"Average_Product_Rating\",\n",
    "#     \"Payment_Method_Preference\",\n",
    "#     \"daywise_trends\",\n",
    "#     \"weekdaywise_trends\",\n",
    "#     \"monthwise_trends\",\n",
    "#     \"yearwise_trends\",\n",
    "#     \"Average_Delivery_Time_by_Shipping_Method\",\n",
    "#     \"customers_purchase_same_product\",\n",
    "#     \"revenue_risk_returns\",\n",
    "#     \"Most_Product_Purchase_Frequency\",\n",
    "#     \"product_purchase_by_Revenue_and_gender\",\n",
    "#     \"Repeat_Purchase_Score_by_Product\",\n",
    "#     \"Brand_Loyalty_Score\",\n",
    "# ]\n",
    "\n",
    "\n",
    "# gold_base_path = \"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/gold_layer/\"\n",
    "\n",
    "base_path = \"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/gold_layer/\"\n",
    "\n",
    "folders = dbutils.fs.ls(base_path)\n",
    "\n",
    "delta_tables = []\n",
    "for f in folders:\n",
    "    if f.isDir():\n",
    "        delta_log_path = f\"{f.path}_delta_log/\"\n",
    "        try:\n",
    "            # If _delta_log folder exists, it's a Delta table\n",
    "            dbutils.fs.ls(delta_log_path)\n",
    "            delta_tables.append(f.name.strip('/'))  # removes trailing slash\n",
    "        except Exception:\n",
    "            pass  # Not a Delta table\n",
    "\n",
    "print(\"Delta Tables found:\", delta_tables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96d3c2e7-a073-4640-a38f-14fc8ea06f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(\n",
    "    user=\"TULSI12\",\n",
    "    password=\"Tulsithakkar12\",\n",
    "    account=\"mxlauud-hz15835\",\n",
    "    warehouse=\"COMPUTE_WH\",\n",
    "    database=\"DBX_INTERNALPROC_DESTINATION\",\n",
    "    schema=\"DBX_SCHEMA\"\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3393c50-51a7-437e-a454-14391eeab8f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDD04 Processing table: Average_Delivery_Time_by_Shipping_Method\nTable AVERAGE_DELIVERY_TIME_BY_SHIPPING_METHOD already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: Average_Product_Rating\nTable AVERAGE_PRODUCT_RATING already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: Brand_Loyalty_Score\nTable BRAND_LOYALTY_SCORE already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: Brand_Return_Rate\nTable BRAND_RETURN_RATE already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: Customer_Retention_Rate\nTable CUSTOMER_RETENTION_RATE already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: Monthly_Churn_Rate\nTable MONTHLY_CHURN_RATE already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: Most_Product_Purchase_Frequency\nTable MOST_PRODUCT_PURCHASE_FREQUENCY already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: Payment_Method_Preference\nTable PAYMENT_METHOD_PREFERENCE already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: Repeat_Purchase_Score_by_Product\nTable REPEAT_PURCHASE_SCORE_BY_PRODUCT already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: customers_purchase_same_product\nTable CUSTOMERS_PURCHASE_SAME_PRODUCT already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: daywise_trends\nTable DAYWISE_TRENDS already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: monthwise_trends\nTable MONTHWISE_TRENDS already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: product_purchase_by_Revenue_and_gender\nExecuting:\nCREATE TABLE PRODUCT_PURCHASE_BY_REVENUE_AND_GENDER (\n  PRODUCT_TYPE VARCHAR,\n  GENDER VARCHAR,\n  AGE_GROUP VARCHAR,\n  TOTAL_REVENUE NUMBER,\n  AVG_RATING FLOAT\n);\n\nCreated table PRODUCT_PURCHASE_BY_REVENUE_AND_GENDER in Snowflake.\n\n\uD83D\uDD04 Processing table: revenue_risk_returns\nTable REVENUE_RISK_RETURNS already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: weekdaywise_trends\nTable WEEKDAYWISE_TRENDS already exists in Snowflake. Skipping creation.\n\n\uD83D\uDD04 Processing table: yearwise_trends\nTable YEARWISE_TRENDS already exists in Snowflake. Skipping creation.\n\n\uD83C\uDF89 All Snowflake table creation checks complete.\n"
     ]
    }
   ],
   "source": [
    "def map_data_type(data_type):\n",
    "    if \"string\" in data_type:\n",
    "        return \"VARCHAR\"\n",
    "    elif \"int\" in data_type:\n",
    "        return \"NUMBER\"\n",
    "    elif \"long\" in data_type:\n",
    "        return \"NUMBER\"\n",
    "    elif \"double\" in data_type or \"float\" in data_type:\n",
    "        return \"FLOAT\"\n",
    "    elif \"timestamp\" in data_type:\n",
    "        return \"TIMESTAMP\"\n",
    "    elif \"date\" in data_type:\n",
    "        return \"DATE\"\n",
    "    else:\n",
    "        return \"VARCHAR\"\n",
    "\n",
    "\n",
    "\n",
    "def table_exists(cursor, table_name):\n",
    "    cursor.execute(f\"SHOW TABLES LIKE '{table_name.upper()}'\")\n",
    "    return cursor.fetchone() is not None\n",
    "\n",
    "\n",
    "for tbl in delta_tables:\n",
    "    try:\n",
    "        print(f\"\uD83D\uDD04 Processing table: {tbl}\")\n",
    "\n",
    "        table_name_upper = tbl.upper()\n",
    "\n",
    "        #Check if table exists in Snowflake\n",
    "        if table_exists(cursor, table_name_upper):\n",
    "            print(f\"Table {table_name_upper} already exists in Snowflake. Skipping creation.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Read Delta table schema\n",
    "        df = spark.read.format(\"delta\").load(f\"{base_path}{tbl}\")\n",
    "        schema = df.schema\n",
    "\n",
    "        # Generate CREATE TABLE statement\n",
    "        ddl = f\"CREATE TABLE {table_name_upper} (\\n\"\n",
    "        for field in schema.fields:\n",
    "            col_name = field.name.upper()\n",
    "            snow_type = map_data_type(field.dataType.simpleString())\n",
    "            ddl += f\"  {col_name} {snow_type},\\n\"\n",
    "        ddl = ddl.rstrip(\",\\n\") + \"\\n);\"\n",
    "\n",
    "        # Execute DDL\n",
    "        print(f\"Executing:\\n{ddl}\\n\")\n",
    "        cursor.execute(ddl)\n",
    "        print(f\"Created table {table_name_upper} in Snowflake.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {tbl}: {e}\")\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"All Snowflake table creation checks complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Snowflake_DDL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}