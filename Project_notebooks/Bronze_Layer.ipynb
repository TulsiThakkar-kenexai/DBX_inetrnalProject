{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a134ca0c-fab0-470d-94f2-f396c36af2f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "LI_s-rc7mOUI"
   },
   "outputs": [],
   "source": [
    "#Databricks intenal Project\n",
    "\n",
    "#External Table loading in Bronze_layer\n",
    "\n",
    "#Service principal configration\n",
    "\n",
    "client_id = \"06c11865-4034-4363-9e84-720eb4487b8b\"\n",
    "tenant_id = \"1c292f40-07c6-4a6a-9106-a006f98c2da1\"\n",
    "client_secret = \"kWN8Q~r3BTjP3DaJ-3VHoDZInJpI5VJ.pNAFkaxm\"\n",
    "storage_account_name = \"dbxinternalproject\"\n",
    "container_name = \"dbxintenalproccontainer\"\n",
    "\n",
    "\n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{storage_account_name}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{storage_account_name}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{storage_account_name}.dfs.core.windows.net\", client_id)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{storage_account_name}.dfs.core.windows.net\", client_secret )\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{storage_account_name}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506c9d2b-a0f4-4de8-a32d-6c4d10ed3255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "RLKAln14mOUJ",
    "outputId": "53229e56-955d-43a6-a8e5-9e955a563686"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/', name='Bronze_layer/', size=0, modificationTime=1749200643000),\n",
       " FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/gold_layer/', name='gold_layer/', size=0, modificationTime=1749200665000),\n",
       " FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/sliver_layer/', name='sliver_layer/', size=0, modificationTime=1749200652000)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f4b4a0a-c8d6-4df0-9ef8-f2120c7775cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNG8X90nmOUK",
    "outputId": "97dd7ec0-a133-4063-bbd0-4840b7737efc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading latest JSON file: abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Raw_data/retail_data_dataset.json\n+------+--------------+--------------------+------+--------------------+--------------------+-------+----------+-------+-------+--------------------+----------------+--------------------+--------------------+--------------+-------------------+------------+----------+-----------------+----------------+--------------------+--------+---------------+----------------+--------+\n|Gender|Payment_Method|          Product_id|Rating|      Transaction_ID|             address| amount|birth_date|   city|country|         customer_id|customer_segment|       datetimestamp|               email|      feedback|               name|order_status|     phone|    product_brand|product_category|        product_type|quantity|shipping_method|           state|zip_code|\n+------+--------------+--------------------+------+--------------------+--------------------+-------+----------+-------+-------+--------------------+----------------+--------------------+--------------------+--------------+-------------------+------------+----------+-----------------+----------------+--------------------+--------+---------------+----------------+--------+\n|Female|        PayPal|1f4cfde85d7e44bdb...|   3.7|44d28f9a60fe4d42a...|Unit 6574 Box 322...|19617.0|10/11/1985|Belfast|     UK|15514bb63ff14c8cb...|         Premium|2024-11-26T09:19:...|acabrera@example.net|Very satisfied|    Anthony Walters|       Order|6547758334|       Home Depot|      Home Decor|      Pendant lights|     3.2|       Same-day|Northern Ireland|   39985|\n|Female|        PayPal|4f409860d43d4d1f9...|   3.7|44d28f9a60fe4d42a...|Unit 6574 Box 322...| 1577.0|10/11/1985|Belfast|     UK|15514bb63ff14c8cb...|         Premium|2024-11-26T09:19:...|acabrera@example.net|Very satisfied|    Anthony Walters|       Order|6547758334|     PenguinBooks|           Books|Psychological thr...|    NULL|       Same-day|Northern Ireland|   39985|\n|Female|        PayPal|1f4cfde85d7e44bdb...|   3.7|44d28f9a60fe4d42a...|Unit 6574 Box 322...|19617.0|10/11/1985|Belfast|     UK|15514bb63ff14c8cb...|         Premium|2024-11-26T09:49:...|acabrera@example.net|Very satisfied|    Anthony Walters|     Shipped|6547758334|       Home Depot|      Home Decor|      Pendant lights|    NULL|       Same-day|Northern Ireland|   39985|\n|Female|        PayPal|4f409860d43d4d1f9...|   3.7|44d28f9a60fe4d42a...|Unit 6574 Box 322...| 1577.0|10/11/1985|Belfast|     UK|15514bb63ff14c8cb...|         Premium|2024-11-26T09:49:...|acabrera@example.net|Very satisfied|    Anthony Walters|     Shipped|6547758334|     PenguinBooks|           Books|Psychological thr...|     2.2|       Same-day|Northern Ireland|   39985|\n|Female|        PayPal|1f4cfde85d7e44bdb...|   3.7|44d28f9a60fe4d42a...|Unit 6574 Box 322...|19617.0|10/11/1985|Belfast|     UK|15514bb63ff14c8cb...|         Premium|2024-11-26T10:19:...|acabrera@example.net|Very satisfied|    Anthony Walters|   Delivered|6547758334|       Home Depot|      Home Decor|      Pendant lights|     3.2|       Same-day|Northern Ireland|   39985|\n|Female|        PayPal|1f4cfde85d7e44bdb...|   3.7|44d28f9a60fe4d42a...|Unit 6574 Box 322...|19617.0|10/11/1985|Belfast|     UK|15514bb63ff14c8cb...|         Premium|2024-11-26T10:49:...|acabrera@example.net|Very satisfied|    Anthony Walters|    Returned|6547758334|       Home Depot|      Home Decor|      Pendant lights|  221.54|       Same-day|Northern Ireland|   39985|\n|Female|          Cash|60deb1b7fe654351b...|   2.8|7dd47f22d89648f3b...|0770 Mark Junctio...| 8175.0|28/11/1995| Berlin|Germany|fca550b18ffb4836b...|             New|2025-02-07T07:00:...|qmcintyre@example...|          NULL|     Charles Fields|       Order|3019401499|           Adidas|        Clothing|            Sneakers|     3.0|        Express|          Berlin|   42306|\n|Female|          Cash|60deb1b7fe654351b...|   2.8|7dd47f22d89648f3b...|0770 Mark Junctio...| 8175.0|28/11/1995| Berlin|Germany|fca550b18ffb4836b...|             New|2025-02-09T04:12:...|qmcintyre@example...|          NULL|     Charles Fields|   Cancelled|3019401499|           Adidas|        Clothing|            Sneakers|     3.0|        Express|          Berlin|   42306|\n|Female|    Debit Card|e28a081446744ed98...|  NULL|c49512d0653246739...|730 Manuel Manors...|18312.0|24/06/1999|Cardiff|     UK|e25fff392b3b4b58b...|             New|2024-07-22T04:36:...| twilcox@example.org|          NULL|   Courtney Ferrell|       Order| 274007313|    HarperCollins|           Books|             Romance|     5.0|        Express|           Wales|   54910|\n|Female|    Debit Card|e28a081446744ed98...|  NULL|c49512d0653246739...|730 Manuel Manors...|18312.0|24/06/1999|Cardiff|     UK|e25fff392b3b4b58b...|             New|2024-07-23T09:29:...| twilcox@example.org|          NULL|   Courtney Ferrell|     Shipped| 274007313|    HarperCollins|           Books|             Romance|     5.0|        Express|           Wales|   54910|\n|Female|    Debit Card|e28a081446744ed98...|  NULL|c49512d0653246739...|730 Manuel Manors...|18312.0|24/06/1999|Cardiff|     UK|e25fff392b3b4b58b...|             New|2024-07-26T06:36:...| twilcox@example.org|          NULL|   Courtney Ferrell|   Cancelled| 274007313|    HarperCollins|           Books|             Romance|     5.0|        Express|           Wales|   54910|\n|Female|        PayPal|e28a081446744ed98...|   3.5|2b6f67d2ee474a248...|472 Medina Plaza ...|18312.0|14/10/1986| Munich|Germany|228c9f46dc3246efa...|             New|2024-09-21T07:25:...|jennifer26@exampl...|          NULL|Michaela Williamson|       Order|2176985344|    HarperCollins|           Books|             Romance|     4.0|       Same-day|         Bavaria|   92060|\n|Female|        PayPal|a11eebe173ef4de79...|   3.5|2b6f67d2ee474a248...|472 Medina Plaza ...| 1900.0|14/10/1986| Munich|Germany|228c9f46dc3246efa...|             New|2024-09-21T07:25:...|jennifer26@exampl...|          NULL|Michaela Williamson|       Order|2176985344|Bed Bath & Beyond|      Home Decor|  Decorative pillows|     3.0|       Same-day|         Bavaria|   92060|\n|Female|        PayPal|e28a081446744ed98...|   3.5|2b6f67d2ee474a248...|472 Medina Plaza ...|18312.0|14/10/1986| Munich|Germany|228c9f46dc3246efa...|             New|2024-09-21T07:55:...|jennifer26@exampl...|          NULL|Michaela Williamson|     Shipped|2176985344|    HarperCollins|           Books|             Romance|     4.0|       Same-day|         Bavaria|   92060|\n|Female|        PayPal|a11eebe173ef4de79...|   3.5|2b6f67d2ee474a248...|472 Medina Plaza ...| 1900.0|14/10/1986| Munich|Germany|228c9f46dc3246efa...|             New|2024-09-21T07:55:...|jennifer26@exampl...|          NULL|Michaela Williamson|     Shipped|2176985344|Bed Bath & Beyond|      Home Decor|  Decorative pillows|     3.0|       Same-day|         Bavaria|   92060|\n|Female|        PayPal|a11eebe173ef4de79...|   3.5|2b6f67d2ee474a248...|472 Medina Plaza ...| 1900.0|14/10/1986| Munich|Germany|228c9f46dc3246efa...|             New|2024-09-21T08:25:...|jennifer26@exampl...|          NULL|Michaela Williamson|   Cancelled|2176985344|Bed Bath & Beyond|      Home Decor|  Decorative pillows|     3.0|       Same-day|         Bavaria|   92060|\n|  Male|          Cash|a11eebe173ef4de79...|   1.4|4572200a6feb43ee9...|2787 David Knoll ...| 1900.0|09/07/1993|Cardiff|     UK|32bc2e7e399341728...|         Regular|2024-06-21T12:14:...|gardnerbridget@ex...|          NULL|     Douglas Sutton|       Order|8371380682|Bed Bath & Beyond|      Home Decor|  Decorative pillows|     5.0|       Same-day|           Wales|   21821|\n|  Male|          Cash|79836fd13a3545239...|   1.4|4572200a6feb43ee9...|2787 David Knoll ...| 7647.0|09/07/1993|Cardiff|     UK|32bc2e7e399341728...|         Regular|2024-06-21T12:14:...|gardnerbridget@ex...|          NULL|     Douglas Sutton|       Order|8371380682|             Nike|        Clothing|             T-shirt|     2.0|       Same-day|           Wales|   21821|\n|  Male|          Cash|6c817f66fa7a4927a...|   1.4|4572200a6feb43ee9...|2787 David Knoll ...|16759.0|09/07/1993|Cardiff|     UK|32bc2e7e399341728...|         Regular|2024-06-21T12:14:...|gardnerbridget@ex...|          NULL|     Douglas Sutton|       Order|8371380682|             Nike|        Clothing|            Sneakers|     1.0|       Same-day|           Wales|   21821|\n|  Male|          Cash|73d2f802e65d4790b...|   1.4|4572200a6feb43ee9...|2787 David Knoll ...|18242.0|09/07/1993|Cardiff|     UK|32bc2e7e399341728...|         Regular|2024-06-21T12:14:...|gardnerbridget@ex...|          NULL|     Douglas Sutton|       Order|8371380682|             IKEA|      Home Decor|  Decorative pillows|     3.0|       Same-day|           Wales|   21821|\n+------+--------------+--------------------+------+--------------------+--------------------+-------+----------+-------+-------+--------------------+----------------+--------------------+--------------------+--------------+-------------------+------------+----------+-----------------+----------------+--------------------+--------+---------------+----------------+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Raw_data\"\n",
    "files = dbutils.fs.ls(folder_path)\n",
    "\n",
    "# Filter only JSON files\n",
    "json_files = [f for f in files if f.name.endswith(\".json\")]\n",
    "\n",
    "# Pick the latest file by modification time\n",
    "latest_file = max(json_files, key=lambda f: f.modificationTime)\n",
    "\n",
    "# Full path of the latest file\n",
    "latest_file_path = latest_file.path\n",
    "print(\"Loading latest JSON file:\", latest_file_path)\n",
    "\n",
    "source_df = spark.read.json(latest_file_path)\n",
    "source_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40a0bda-2d9c-48f2-a45a-1c3c25d0a668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import uuid\n",
    "from pyspark.sql.functions import col, current_timestamp,lit\n",
    "random_batch_id = str(uuid.uuid4())[:10]\n",
    "source_df_updated = source_df.withColumn(\"Batch_id\", lit(random_batch_id))\\\n",
    "                      .withColumn(\"insert_timestamp\", current_timestamp())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4d48935-c4d8-49cf-8e2b-fe938cb4325f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n|  Batch_id|    insert_timestamp|\n+----------+--------------------+\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n|b9fe1ab3-8|2025-06-09 10:21:...|\n+----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "source_df_updated.select('Batch_id','insert_timestamp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029e6512-7c77-4d8b-9602-aea038f90d4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external\",recurse=True)\n",
    "# dbutils.fs.rm(\"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Batch_tracking_tbl\",recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa3f094-33ba-4e6a-9c0d-f2b767530b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "NyeHIKDumOUK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#source_df.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "delta_path = \"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external\" \n",
    "source_df_updated.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").save(delta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6655403a-b1ea-445c-968b-3f305563e09e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Batch_tracking_tbl/', name='Batch_tracking_tbl/', size=0, modificationTime=1749233159000),\n",
       " FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Raw_data/', name='Raw_data/', size=0, modificationTime=1749232994000),\n",
       " FileInfo(path='abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/', name='retail_data_external/', size=0, modificationTime=1749230840000)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.ls(f\"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a23c8942-fdcb-4cb4-9b85-6d6ec0f6967c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#delta_path = \"/dbfs/tmp/bronze_layer/retail_data_external\"  #temp location-managed table\n",
    "# delta_path = \"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/\"  #main_location_unmanaged table \n",
    "delta_path = \"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/\" \n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dbx_internalproc.bronze_layer.retail_data_external\n",
    "    USING DELTA\n",
    "    LOCATION '{delta_path}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d53a992-9858-4182-a633-b7017ae3fc45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uwxN8uGYmOUL",
    "outputId": "88935eab-a6d4-487c-db13-ea33bc1e226b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+------+--------------------+--------------------+-------+----------+-------+-------+--------------------+----------------+--------------------+--------------------+--------------------+-------------+------------+----------+-----------------+----------------+--------------------+--------+---------------+----------------+--------+----------+--------------------+\n|Gender|Payment_Method|          Product_id|Rating|      Transaction_ID|             address| amount|birth_date|   city|country|         customer_id|customer_segment|       datetimestamp|               email|            feedback|         name|order_status|     phone|    product_brand|product_category|        product_type|quantity|shipping_method|           state|zip_code|  Batch_id|    insert_timestamp|\n+------+--------------+--------------------+------+--------------------+--------------------+-------+----------+-------+-------+--------------------+----------------+--------------------+--------------------+--------------------+-------------+------------+----------+-----------------+----------------+--------------------+--------+---------------+----------------+--------+----------+--------------------+\n|Female|    Debit Card|900e1d36cf7f4ac1b...|   1.7|44d4ad141f3349ecb...|45250 Weber Ford ...|14692.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f8...|         Premium|2024-12-23T22:40:...|qnichols@example.net|                NULL| Brenda Ponce|       Order|1506901228|        Coca-Cola|         Grocery|   Chocolate cookies|     5.0|       Standard|         Hamburg|   74815|86997d14-1|2025-06-09 09:57:...|\n|Female|    Debit Card|6413366fed6343758...|   1.7|44d4ad141f3349ecb...|45250 Weber Ford ...| 8876.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f8...|         Premium|2024-12-23T22:40:...|qnichols@example.net|                NULL| Brenda Ponce|       Order|1506901228|           Amazon|     Electronics|  Amazon Fire Tablet|     3.0|       Standard|         Hamburg|   74815|86997d14-1|2025-06-09 09:57:...|\n|Female|    Debit Card|14babfe330224d9b8...|   1.7|44d4ad141f3349ecb...|45250 Weber Ford ...| 2197.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f8...|         Premium|2024-12-24T07:23:...|qnichols@example.net|                NULL| Brenda Ponce|     Shipped|1506901228|     Random House|           Books|Psychological thr...|     1.0|       Standard|         Hamburg|   74815|86997d14-1|2025-06-09 09:57:...|\n|Female|    Debit Card|900e1d36cf7f4ac1b...|   1.7|44d4ad141f3349ecb...|45250 Weber Ford ...|14692.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f8...|         Premium|2024-12-25T01:15:...|qnichols@example.net|                NULL| Brenda Ponce|     Shipped|1506901228|        Coca-Cola|         Grocery|   Chocolate cookies|     5.0|       Standard|         Hamburg|   74815|86997d14-1|2025-06-09 09:57:...|\n|Female|    Debit Card|900e1d36cf7f4ac1b...|   1.7|44d4ad141f3349ecb...|45250 Weber Ford ...|14692.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f8...|         Premium|2024-12-25T20:36:...|qnichols@example.net|                NULL| Brenda Ponce|   Cancelled|1506901228|        Coca-Cola|         Grocery|   Chocolate cookies|     5.0|       Standard|         Hamburg|   74815|86997d14-1|2025-06-09 09:57:...|\n|Female|    Debit Card|aff9792deabc4ecfb...|   1.7|44d4ad141f3349ecb...|45250 Weber Ford ...| 6368.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f8...|         Premium|2024-12-26T08:27:...|qnichols@example.net|                NULL| Brenda Ponce|     Shipped|1506901228|            Apple|     Electronics|                iPad|     1.0|       Standard|         Hamburg|   74815|86997d14-1|2025-06-09 09:57:...|\n|Female|    Debit Card|14babfe330224d9b8...|   1.7|44d4ad141f3349ecb...|45250 Weber Ford ...| 2197.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f8...|         Premium|2024-12-27T04:18:...|qnichols@example.net|                NULL| Brenda Ponce|   Cancelled|1506901228|     Random House|           Books|Psychological thr...|     1.0|       Standard|         Hamburg|   74815|86997d14-1|2025-06-09 09:57:...|\n|Female|    Debit Card|6413366fed6343758...|   1.7|44d4ad141f3349ecb...|45250 Weber Ford ...| 8876.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f8...|         Premium|2024-12-27T06:24:...|qnichols@example.net|                NULL| Brenda Ponce|   Cancelled|1506901228|           Amazon|     Electronics|  Amazon Fire Tablet|     3.0|       Standard|         Hamburg|   74815|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|a11eebe173ef4de79...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...| 1900.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T20:40:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|       Order|3884289246|Bed Bath & Beyond|      Home Decor|  Decorative pillows|     4.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|e28a081446744ed98...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...|18312.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T20:40:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|       Order|3884289246|    HarperCollins|           Books|             Romance|     2.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|aace6debaa4049c68...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...| 7105.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T20:40:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|       Order|3884289246|             Zara|        Clothing|          Polo shirt|     5.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|e28a081446744ed98...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...|18312.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T21:10:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|     Shipped|3884289246|    HarperCollins|           Books|             Romance|     2.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|a11eebe173ef4de79...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...| 1900.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T21:10:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|     Shipped|3884289246|Bed Bath & Beyond|      Home Decor|  Decorative pillows|     4.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|aace6debaa4049c68...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...| 7105.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T21:10:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|     Shipped|3884289246|             Zara|        Clothing|          Polo shirt|     5.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|a11eebe173ef4de79...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...| 1900.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T21:40:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|   Delivered|3884289246|Bed Bath & Beyond|      Home Decor|  Decorative pillows|     4.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|e28a081446744ed98...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...|18312.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T21:40:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|   Delivered|3884289246|    HarperCollins|           Books|             Romance|     2.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|a11eebe173ef4de79...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...| 1900.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T22:10:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|    Returned|3884289246|Bed Bath & Beyond|      Home Decor|  Decorative pillows|     4.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|Female|          Cash|e28a081446744ed98...|  NULL|ac9487a8d3c24e519...|03166 Aaron Land ...|18312.0|25/05/2004|Belfast|     UK|1db94120012b443c9...|         Regular|2024-10-26T22:10:...|macdonaldcassandr...| Would not recommend|Bradley Bauer|    Returned|3884289246|    HarperCollins|           Books|             Romance|     2.0|       Same-day|Northern Ireland|   91434|86997d14-1|2025-06-09 09:57:...|\n|  Male|          Cash|73d2f802e65d4790b...|  NULL|c0b7bc1f4b464de4b...|9955 Johnson Miss...|18242.0|31/03/1999| Berlin|Germany|7d5b9c4be7b14648a...|         Regular|2025-04-18T08:45:...| iharris@example.com|Good value for money|Matthew Smith|       Order|7753802446|             IKEA|      Home Decor|  Decorative pillows|     1.0|       Same-day|          Berlin|   76737|86997d14-1|2025-06-09 09:57:...|\n|  Male|          Cash|aff9792deabc4ecfb...|  NULL|c0b7bc1f4b464de4b...|9955 Johnson Miss...| 6368.0|31/03/1999| Berlin|Germany|7d5b9c4be7b14648a...|         Regular|2025-04-18T08:45:...| iharris@example.com|Good value for money|Matthew Smith|       Order|7753802446|            Apple|     Electronics|                iPad|     1.0|       Same-day|          Berlin|   76737|86997d14-1|2025-06-09 09:57:...|\n+------+--------------+--------------------+------+--------------------+--------------------+-------+----------+-------+-------+--------------------+----------------+--------------------+--------------------+--------------------+-------------+------------+----------+-----------------+----------------+--------------------+--------+---------------+----------------+--------+----------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    select * from dbx_internalproc.bronze_layer.retail_data_external\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d4d3d9-f3c4-4efb-a897-269f310ac491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "6C__U8WSmOUL",
    "outputId": "6374d81a-b834-496f-9736-018915e9431f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------------+------+----------------------+-------------------------------------------------------+-------+----------+-------+-------+----------------------+----------------+------------------------+------------------------------+--------------------+-------------+------------+----------+-----------------+----------------+----------------------+--------+---------------+----------------+--------+----------+-----------------------+\n|Gender|Payment_Method|Product_id            |Rating|Transaction_ID        |address                                                |amount |birth_date|city   |country|customer_id           |customer_segment|datetimestamp           |email                         |feedback            |name         |order_status|phone     |product_brand    |product_category|product_type          |quantity|shipping_method|state           |zip_code|Batch_id  |insert_timestamp       |\n+------+--------------+----------------------+------+----------------------+-------------------------------------------------------+-------+----------+-------+-------+----------------------+----------------+------------------------+------------------------------+--------------------+-------------+------------+----------+-----------------+----------------+----------------------+--------+---------------+----------------+--------+----------+-----------------------+\n|Female|Debit Card    |900e1d36cf7f4ac1bae2ed|1.7   |44d4ad141f3349ecbe8904|45250 Weber Ford Suite 821, New Robert, NM 74815       |14692.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f83eacb|Premium         |2024-12-23T22:40:16.000Z|qnichols@example.net          |NULL                |Brenda Ponce |Order       |1506901228|Coca-Cola        |Grocery         |Chocolate cookies     |5.0     |Standard       |Hamburg         |74815   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Debit Card    |6413366fed6343758da3e4|1.7   |44d4ad141f3349ecbe8904|45250 Weber Ford Suite 821, New Robert, NM 74815       |8876.0 |25/05/2006|Hamburg|Germany|0a30652f973d4b9f83eacb|Premium         |2024-12-23T22:40:16.000Z|qnichols@example.net          |NULL                |Brenda Ponce |Order       |1506901228|Amazon           |Electronics     |Amazon Fire Tablet    |3.0     |Standard       |Hamburg         |74815   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Debit Card    |14babfe330224d9b8583bb|1.7   |44d4ad141f3349ecbe8904|45250 Weber Ford Suite 821, New Robert, NM 74815       |2197.0 |25/05/2006|Hamburg|Germany|0a30652f973d4b9f83eacb|Premium         |2024-12-24T07:23:16.000Z|qnichols@example.net          |NULL                |Brenda Ponce |Shipped     |1506901228|Random House     |Books           |Psychological thriller|1.0     |Standard       |Hamburg         |74815   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Debit Card    |900e1d36cf7f4ac1bae2ed|1.7   |44d4ad141f3349ecbe8904|45250 Weber Ford Suite 821, New Robert, NM 74815       |14692.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f83eacb|Premium         |2024-12-25T01:15:16.000Z|qnichols@example.net          |NULL                |Brenda Ponce |Shipped     |1506901228|Coca-Cola        |Grocery         |Chocolate cookies     |5.0     |Standard       |Hamburg         |74815   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Debit Card    |900e1d36cf7f4ac1bae2ed|1.7   |44d4ad141f3349ecbe8904|45250 Weber Ford Suite 821, New Robert, NM 74815       |14692.0|25/05/2006|Hamburg|Germany|0a30652f973d4b9f83eacb|Premium         |2024-12-25T20:36:16.000Z|qnichols@example.net          |NULL                |Brenda Ponce |Cancelled   |1506901228|Coca-Cola        |Grocery         |Chocolate cookies     |5.0     |Standard       |Hamburg         |74815   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Debit Card    |aff9792deabc4ecfbd5074|1.7   |44d4ad141f3349ecbe8904|45250 Weber Ford Suite 821, New Robert, NM 74815       |6368.0 |25/05/2006|Hamburg|Germany|0a30652f973d4b9f83eacb|Premium         |2024-12-26T08:27:16.000Z|qnichols@example.net          |NULL                |Brenda Ponce |Shipped     |1506901228|Apple            |Electronics     |iPad                  |1.0     |Standard       |Hamburg         |74815   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Debit Card    |14babfe330224d9b8583bb|1.7   |44d4ad141f3349ecbe8904|45250 Weber Ford Suite 821, New Robert, NM 74815       |2197.0 |25/05/2006|Hamburg|Germany|0a30652f973d4b9f83eacb|Premium         |2024-12-27T04:18:16.000Z|qnichols@example.net          |NULL                |Brenda Ponce |Cancelled   |1506901228|Random House     |Books           |Psychological thriller|1.0     |Standard       |Hamburg         |74815   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Debit Card    |6413366fed6343758da3e4|1.7   |44d4ad141f3349ecbe8904|45250 Weber Ford Suite 821, New Robert, NM 74815       |8876.0 |25/05/2006|Hamburg|Germany|0a30652f973d4b9f83eacb|Premium         |2024-12-27T06:24:16.000Z|qnichols@example.net          |NULL                |Brenda Ponce |Cancelled   |1506901228|Amazon           |Electronics     |Amazon Fire Tablet    |3.0     |Standard       |Hamburg         |74815   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |a11eebe173ef4de79e7ae4|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|1900.0 |25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T20:40:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Order       |3884289246|Bed Bath & Beyond|Home Decor      |Decorative pillows    |4.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |e28a081446744ed98cfc78|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|18312.0|25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T20:40:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Order       |3884289246|HarperCollins    |Books           |Romance               |2.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |aace6debaa4049c687bb85|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|7105.0 |25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T20:40:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Order       |3884289246|Zara             |Clothing        |Polo shirt            |5.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |e28a081446744ed98cfc78|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|18312.0|25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T21:10:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Shipped     |3884289246|HarperCollins    |Books           |Romance               |2.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |a11eebe173ef4de79e7ae4|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|1900.0 |25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T21:10:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Shipped     |3884289246|Bed Bath & Beyond|Home Decor      |Decorative pillows    |4.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |aace6debaa4049c687bb85|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|7105.0 |25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T21:10:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Shipped     |3884289246|Zara             |Clothing        |Polo shirt            |5.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |a11eebe173ef4de79e7ae4|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|1900.0 |25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T21:40:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Delivered   |3884289246|Bed Bath & Beyond|Home Decor      |Decorative pillows    |4.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |e28a081446744ed98cfc78|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|18312.0|25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T21:40:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Delivered   |3884289246|HarperCollins    |Books           |Romance               |2.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |a11eebe173ef4de79e7ae4|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|1900.0 |25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T22:10:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Returned    |3884289246|Bed Bath & Beyond|Home Decor      |Decorative pillows    |4.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Female|Cash          |e28a081446744ed98cfc78|NULL  |ac9487a8d3c24e51977b1b|03166 Aaron Land Suite 974, North Michaelview, LA 91434|18312.0|25/05/2004|Belfast|UK     |1db94120012b443c9d9f6d|Regular         |2024-10-26T22:10:39.000Z|macdonaldcassandra@example.com|Would not recommend |Bradley Bauer|Returned    |3884289246|HarperCollins    |Books           |Romance               |2.0     |Same-day       |Northern Ireland|91434   |86997d14-1|2025-06-09 09:57:40.065|\n|Male  |Cash          |73d2f802e65d4790bb5a5c|NULL  |c0b7bc1f4b464de4b14cd1|9955 Johnson Mission Suite 046, New Troyview, PR 76737 |18242.0|31/03/1999|Berlin |Germany|7d5b9c4be7b14648add420|Regular         |2025-04-18T08:45:06.000Z|iharris@example.com           |Good value for money|Matthew Smith|Order       |7753802446|IKEA             |Home Decor      |Decorative pillows    |1.0     |Same-day       |Berlin          |76737   |86997d14-1|2025-06-09 09:57:40.065|\n|Male  |Cash          |aff9792deabc4ecfbd5074|NULL  |c0b7bc1f4b464de4b14cd1|9955 Johnson Mission Suite 046, New Troyview, PR 76737 |6368.0 |31/03/1999|Berlin |Germany|7d5b9c4be7b14648add420|Regular         |2025-04-18T08:45:06.000Z|iharris@example.com           |Good value for money|Matthew Smith|Order       |7753802446|Apple            |Electronics     |iPad                  |1.0     |Same-day       |Berlin          |76737   |86997d14-1|2025-06-09 09:57:40.065|\n+------+--------------+----------------------+------+----------------------+-------------------------------------------------------+-------+----------+-------+-------+----------------------+----------------+------------------------+------------------------------+--------------------+-------------+------------+----------+-----------------+----------------+----------------------+--------+---------------+----------------+--------+----------+-----------------------+\nonly showing top 20 rows\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2958408"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spark.sql(\"SELECT * FROM default.retail_data_external\").show(truncate=False)\n",
    "# source_df = spark.table(\"bronze_layer.retail_data_external\")\n",
    "source_df_main = spark.table(\"dbx_internalproc.bronze_layer.retail_data_external\")   \n",
    "source_df_main.show(truncate=False)\n",
    "source_df_main.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61f6ae16-6067-4e70-a508-9815c0c33315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3j-MHbDqo4Ar"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "source_df_main = source_df_main.select('Batch_id','insert_timestamp').distinct().orderBy(col('insert_timestamp').desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b17d51-1477-46fa-abc9-f26a692c084c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_df_main= source_df_main.withColumn('Processed_flag',lit('N'))\n",
    "# source_df_main.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827308e1-b6f4-4bb9-9fbd-bbc8ba02e625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-473407922245576>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m delta_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Batch_tracking_tbl\u001B[39m\u001B[38;5;124m\"\u001B[39m \n",
       "\u001B[0;32m----> 2\u001B[0m source_df_main\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmergeSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(delta_path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o647.save.\n",
       ": org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet. [DELTA_SHALLOW_CLONE_FILE_NOT_FOUND] File abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table `DELETE` statement. This table appears to be a shallow clone, if that is the case, this error can occur when the original table from which this table was cloned has deleted a file that the clone is still using. If you want any clones to be independent of the original table, use a DEEP clone instead. SQLSTATE: KD001\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors.fileNotExistErrorDBR(QueryExecutionErrors.scala)\n",
       "\tat com.databricks.photon.NativeIOBroker.unwrapExecutionException(NativeIOBroker.java:441)\n",
       "\tat com.databricks.photon.NativeIOBroker.access$200(NativeIOBroker.java:108)\n",
       "\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:209)\n",
       "\tat com.databricks.photon.NativeIOBroker.consumeReadFooterRequest(NativeIOBroker.java:599)\n",
       "\tat 0xc7c42c2 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:163)\n",
       "\tat 0x7d00e74 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:258)\n",
       "\tat 0x769c93f <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:321)\n",
       "\tat 0x769c93f <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:482)\n",
       "\tat 0x769af77 <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:466)\n",
       "\tat 0x769ab09 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:372)\n",
       "\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:96)\n",
       "\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:170)\n",
       "\tat com.databricks.photon.JniApiImpl.open(Native Method)\n",
       "\tat com.databricks.photon.JniApi.open(JniApi.scala)\n",
       "\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:74)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:905)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:898)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:905)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:233)\n",
       "\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n",
       "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:92)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:87)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:225)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1045)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:111)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1048)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:935)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbxinternalproject.dfs.core.windows.net/dbxintenalproccontainer/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet?timeout=90&st=2025-06-09T09:33:13Z&sv=2020-02-10&ske=2025-06-09T11:33:13Z&sig=XXXXX&sktid=1c292f40-07c6-4a6a-9106-a006f98c2da1&se=2025-06-09T11:15:37Z&sdd=2&skoid=e5ca8efa-2529-4105XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-06-09T09:33:13Z&sp=rl&skv=2025-01-05&sr=d, PathNotFound, , \"The specified path does not exist. RequestId:6dd9ea1a-d01f-001a-2a28-d9fba0000000 Time:2025-06-09T10:21:51.6995264Z\"\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:684)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:633)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:423)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:360)\n",
       "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
       "\tat com.databricks.common.filesystem.LokiAbfsInputStream.$anonfun$read$3(LokiABFS.scala:219)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.common.filesystem.LokiAbfsInputStream.withExceptionRewrites(LokiABFS.scala:209)\n",
       "\tat com.databricks.common.filesystem.LokiAbfsInputStream.read(LokiABFS.scala:219)\n",
       "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
       "\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:107)\n",
       "\tat com.databricks.sql.io.PendingFutures.lambda$submit$0(PendingFutures.java:71)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n",
       "\t... 3 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o647.save.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet. [DELTA_SHALLOW_CLONE_FILE_NOT_FOUND] File abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table `DELETE` statement. This table appears to be a shallow clone, if that is the case, this error can occur when the original table from which this table was cloned has deleted a file that the clone is still using. If you want any clones to be independent of the original table, use a DEEP clone instead. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.fileNotExistErrorDBR(QueryExecutionErrors.scala)\n\tat com.databricks.photon.NativeIOBroker.unwrapExecutionException(NativeIOBroker.java:441)\n\tat com.databricks.photon.NativeIOBroker.access$200(NativeIOBroker.java:108)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:209)\n\tat com.databricks.photon.NativeIOBroker.consumeReadFooterRequest(NativeIOBroker.java:599)\n\tat 0xc7c42c2 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:163)\n\tat 0x7d00e74 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:258)\n\tat 0x769c93f <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:321)\n\tat 0x769c93f <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:482)\n\tat 0x769af77 <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:466)\n\tat 0x769ab09 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:372)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:96)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:170)\n\tat com.databricks.photon.JniApiImpl.open(Native Method)\n\tat com.databricks.photon.JniApi.open(JniApi.scala)\n\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:74)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:898)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:905)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:233)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:92)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:87)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:225)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1045)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:111)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1048)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:935)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbxinternalproject.dfs.core.windows.net/dbxintenalproccontainer/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet?timeout=90&st=2025-06-09T09:33:13Z&sv=2020-02-10&ske=2025-06-09T11:33:13Z&sig=XXXXX&sktid=1c292f40-07c6-4a6a-9106-a006f98c2da1&se=2025-06-09T11:15:37Z&sdd=2&skoid=e5ca8efa-2529-4105XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-06-09T09:33:13Z&sp=rl&skv=2025-01-05&sr=d, PathNotFound, , \"The specified path does not exist. RequestId:6dd9ea1a-d01f-001a-2a28-d9fba0000000 Time:2025-06-09T10:21:51.6995264Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:684)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:633)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:423)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:360)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.$anonfun$read$3(LokiABFS.scala:219)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.withExceptionRewrites(LokiABFS.scala:209)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.read(LokiABFS.scala:219)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:107)\n\tat com.databricks.sql.io.PendingFutures.lambda$submit$0(PendingFutures.java:71)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\t... 3 more\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o647.save.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet. [DELTA_SHALLOW_CLONE_FILE_NOT_FOUND] File abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table `DELETE` statement. This table appears to be a shallow clone, if that is the case, this error can occur when the original table from which this table was cloned has deleted a file that the clone is still using. If you want any clones to be independent of the original table, use a DEEP clone instead. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.fileNotExistErrorDBR(QueryExecutionErrors.scala)\n\tat com.databricks.photon.NativeIOBroker.unwrapExecutionException(NativeIOBroker.java:441)\n\tat com.databricks.photon.NativeIOBroker.access$200(NativeIOBroker.java:108)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:209)\n\tat com.databricks.photon.NativeIOBroker.consumeReadFooterRequest(NativeIOBroker.java:599)\n\tat 0xc7c42c2 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:163)\n\tat 0x7d00e74 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:258)\n\tat 0x769c93f <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:321)\n\tat 0x769c93f <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:482)\n\tat 0x769af77 <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:466)\n\tat 0x769ab09 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:372)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:96)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:170)\n\tat com.databricks.photon.JniApiImpl.open(Native Method)\n\tat com.databricks.photon.JniApi.open(JniApi.scala)\n\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:74)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:898)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:905)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:233)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:92)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:87)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:225)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1045)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:111)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1048)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:935)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbxinternalproject.dfs.core.windows.net/dbxintenalproccontainer/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet?timeout=90&st=2025-06-09T09:33:13Z&sv=2020-02-10&ske=2025-06-09T11:33:13Z&sig=XXXXX&sktid=1c292f40-07c6-4a6a-9106-a006f98c2da1&se=2025-06-09T11:15:37Z&sdd=2&skoid=e5ca8efa-2529-4105XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-06-09T09:33:13Z&sp=rl&skv=2025-01-05&sr=d, PathNotFound, , \"The specified path does not exist. RequestId:6dd9ea1a-d01f-001a-2a28-d9fba0000000 Time:2025-06-09T10:21:51.6995264Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:684)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:633)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:423)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:360)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.$anonfun$read$3(LokiABFS.scala:219)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.withExceptionRewrites(LokiABFS.scala:209)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.read(LokiABFS.scala:219)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:107)\n\tat com.databricks.sql.io.PendingFutures.lambda$submit$0(PendingFutures.java:71)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\t... 3 more\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-473407922245576>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m delta_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Batch_tracking_tbl\u001B[39m\u001B[38;5;124m\"\u001B[39m \n\u001B[0;32m----> 2\u001B[0m source_df_main\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmergeSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(delta_path)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o647.save.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet. [DELTA_SHALLOW_CLONE_FILE_NOT_FOUND] File abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table `DELETE` statement. This table appears to be a shallow clone, if that is the case, this error can occur when the original table from which this table was cloned has deleted a file that the clone is still using. If you want any clones to be independent of the original table, use a DEEP clone instead. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.fileNotExistErrorDBR(QueryExecutionErrors.scala)\n\tat com.databricks.photon.NativeIOBroker.unwrapExecutionException(NativeIOBroker.java:441)\n\tat com.databricks.photon.NativeIOBroker.access$200(NativeIOBroker.java:108)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:209)\n\tat com.databricks.photon.NativeIOBroker.consumeReadFooterRequest(NativeIOBroker.java:599)\n\tat 0xc7c42c2 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:163)\n\tat 0x7d00e74 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:258)\n\tat 0x769c93f <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:321)\n\tat 0x769c93f <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:482)\n\tat 0x769af77 <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:466)\n\tat 0x769ab09 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:372)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:96)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:170)\n\tat com.databricks.photon.JniApiImpl.open(Native Method)\n\tat com.databricks.photon.JniApi.open(JniApi.scala)\n\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:74)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:898)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:905)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:233)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:92)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:87)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:225)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1045)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:111)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1048)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:935)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbxinternalproject.dfs.core.windows.net/dbxintenalproccontainer/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet?timeout=90&st=2025-06-09T09:33:13Z&sv=2020-02-10&ske=2025-06-09T11:33:13Z&sig=XXXXX&sktid=1c292f40-07c6-4a6a-9106-a006f98c2da1&se=2025-06-09T11:15:37Z&sdd=2&skoid=e5ca8efa-2529-4105XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-06-09T09:33:13Z&sp=rl&skv=2025-01-05&sr=d, PathNotFound, , \"The specified path does not exist. RequestId:6dd9ea1a-d01f-001a-2a28-d9fba0000000 Time:2025-06-09T10:21:51.6995264Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:684)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:633)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:423)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:360)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.$anonfun$read$3(LokiABFS.scala:219)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.withExceptionRewrites(LokiABFS.scala:209)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.read(LokiABFS.scala:219)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:107)\n\tat com.databricks.sql.io.PendingFutures.lambda$submit$0(PendingFutures.java:71)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\t... 3 more\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "delta_path = \"abfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Batch_tracking_tbl\" \n",
    "source_df_main.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\",\"true\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b124c3a-6571-43e3-bbde-48eab7d1c16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-473407922245576>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m delta_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Batch_tracking_tbl\u001B[39m\u001B[38;5;124m\"\u001B[39m \n",
       "\u001B[0;32m----> 2\u001B[0m source_df_main\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmergeSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(delta_path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o647.save.\n",
       ": org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet. [DELTA_SHALLOW_CLONE_FILE_NOT_FOUND] File abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table `DELETE` statement. This table appears to be a shallow clone, if that is the case, this error can occur when the original table from which this table was cloned has deleted a file that the clone is still using. If you want any clones to be independent of the original table, use a DEEP clone instead. SQLSTATE: KD001\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors.fileNotExistErrorDBR(QueryExecutionErrors.scala)\n",
       "\tat com.databricks.photon.NativeIOBroker.unwrapExecutionException(NativeIOBroker.java:441)\n",
       "\tat com.databricks.photon.NativeIOBroker.access$200(NativeIOBroker.java:108)\n",
       "\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:209)\n",
       "\tat com.databricks.photon.NativeIOBroker.consumeReadFooterRequest(NativeIOBroker.java:599)\n",
       "\tat 0xc7c42c2 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:163)\n",
       "\tat 0x7d00e74 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:258)\n",
       "\tat 0x769c93f <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:321)\n",
       "\tat 0x769c93f <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:482)\n",
       "\tat 0x769af77 <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:466)\n",
       "\tat 0x769ab09 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:372)\n",
       "\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:96)\n",
       "\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:170)\n",
       "\tat com.databricks.photon.JniApiImpl.open(Native Method)\n",
       "\tat com.databricks.photon.JniApi.open(JniApi.scala)\n",
       "\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:74)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:905)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n",
       "\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:898)\n",
       "\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:905)\n",
       "\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:233)\n",
       "\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n",
       "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:92)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:87)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:225)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1045)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:111)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1048)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:935)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbxinternalproject.dfs.core.windows.net/dbxintenalproccontainer/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet?timeout=90&st=2025-06-09T09:33:13Z&sv=2020-02-10&ske=2025-06-09T11:33:13Z&sig=XXXXX&sktid=1c292f40-07c6-4a6a-9106-a006f98c2da1&se=2025-06-09T11:15:37Z&sdd=2&skoid=e5ca8efa-2529-4105XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-06-09T09:33:13Z&sp=rl&skv=2025-01-05&sr=d, PathNotFound, , \"The specified path does not exist. RequestId:6dd9ea1a-d01f-001a-2a28-d9fba0000000 Time:2025-06-09T10:21:51.6995264Z\"\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:684)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:633)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:423)\n",
       "\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:360)\n",
       "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
       "\tat com.databricks.common.filesystem.LokiAbfsInputStream.$anonfun$read$3(LokiABFS.scala:219)\n",
       "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
       "\tat com.databricks.common.filesystem.LokiAbfsInputStream.withExceptionRewrites(LokiABFS.scala:209)\n",
       "\tat com.databricks.common.filesystem.LokiAbfsInputStream.read(LokiABFS.scala:219)\n",
       "\tat java.io.DataInputStream.read(DataInputStream.java:149)\n",
       "\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:107)\n",
       "\tat com.databricks.sql.io.PendingFutures.lambda$submit$0(PendingFutures.java:71)\n",
       "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n",
       "\t... 3 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o647.save.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet. [DELTA_SHALLOW_CLONE_FILE_NOT_FOUND] File abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table `DELETE` statement. This table appears to be a shallow clone, if that is the case, this error can occur when the original table from which this table was cloned has deleted a file that the clone is still using. If you want any clones to be independent of the original table, use a DEEP clone instead. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.fileNotExistErrorDBR(QueryExecutionErrors.scala)\n\tat com.databricks.photon.NativeIOBroker.unwrapExecutionException(NativeIOBroker.java:441)\n\tat com.databricks.photon.NativeIOBroker.access$200(NativeIOBroker.java:108)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:209)\n\tat com.databricks.photon.NativeIOBroker.consumeReadFooterRequest(NativeIOBroker.java:599)\n\tat 0xc7c42c2 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:163)\n\tat 0x7d00e74 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:258)\n\tat 0x769c93f <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:321)\n\tat 0x769c93f <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:482)\n\tat 0x769af77 <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:466)\n\tat 0x769ab09 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:372)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:96)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:170)\n\tat com.databricks.photon.JniApiImpl.open(Native Method)\n\tat com.databricks.photon.JniApi.open(JniApi.scala)\n\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:74)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:898)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:905)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:233)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:92)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:87)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:225)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1045)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:111)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1048)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:935)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbxinternalproject.dfs.core.windows.net/dbxintenalproccontainer/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet?timeout=90&st=2025-06-09T09:33:13Z&sv=2020-02-10&ske=2025-06-09T11:33:13Z&sig=XXXXX&sktid=1c292f40-07c6-4a6a-9106-a006f98c2da1&se=2025-06-09T11:15:37Z&sdd=2&skoid=e5ca8efa-2529-4105XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-06-09T09:33:13Z&sp=rl&skv=2025-01-05&sr=d, PathNotFound, , \"The specified path does not exist. RequestId:6dd9ea1a-d01f-001a-2a28-d9fba0000000 Time:2025-06-09T10:21:51.6995264Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:684)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:633)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:423)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:360)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.$anonfun$read$3(LokiABFS.scala:219)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.withExceptionRewrites(LokiABFS.scala:209)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.read(LokiABFS.scala:219)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:107)\n\tat com.databricks.sql.io.PendingFutures.lambda$submit$0(PendingFutures.java:71)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\t... 3 more\n"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-473407922245576>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m delta_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mabfss://dbxintenalproccontainer@dbxinternalproject.dfs.core.windows.net/Bronze_layer/Batch_tracking_tbl\u001B[39m\u001B[38;5;124m\"\u001B[39m \n\u001B[0;32m----> 2\u001B[0m source_df_main\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmergeSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msave(delta_path)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1732\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1730\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1731\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1732\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave(path)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o647.save.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet. [DELTA_SHALLOW_CLONE_FILE_NOT_FOUND] File abfss:REDACTED_LOCAL_PART@dbxinternalproject.dfs.core.windows.net/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet referenced in the transaction log cannot be found. This can occur when data has been manually deleted from the file system rather than using the table `DELETE` statement. This table appears to be a shallow clone, if that is the case, this error can occur when the original table from which this table was cloned has deleted a file that the clone is still using. If you want any clones to be independent of the original table, use a DEEP clone instead. SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.fileNotExistErrorDBR(QueryExecutionErrors.scala)\n\tat com.databricks.photon.NativeIOBroker.unwrapExecutionException(NativeIOBroker.java:441)\n\tat com.databricks.photon.NativeIOBroker.access$200(NativeIOBroker.java:108)\n\tat com.databricks.photon.NativeIOBroker$TaggedIOFutureContainer.getAndApply(NativeIOBroker.java:209)\n\tat com.databricks.photon.NativeIOBroker.consumeReadFooterRequest(NativeIOBroker.java:599)\n\tat 0xc7c42c2 <photon>.RecordMissingOrCorruptFile(external/workspace_spark_3_5/photon/jni-wrappers/jni-io-broker.cc:163)\n\tat 0x7d00e74 <photon>.TryAndMaybeSkipFileOnError(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:258)\n\tat 0x769c93f <photon>.CreateAndOpenFileReaders(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:321)\n\tat 0x769c93f <photon>.OpenFileBatch(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:482)\n\tat 0x769af77 <photon>.DoOpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:466)\n\tat 0x769ab09 <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/file-scan-node.cc:372)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/grouping-agg-node.cc:96)\n\tat 0x75ee6ec <photon>.OpenImpl(external/workspace_spark_3_5/photon/exec-nodes/shuffle-sink-node.cc:170)\n\tat com.databricks.photon.JniApiImpl.open(Native Method)\n\tat com.databricks.photon.JniApi.open(JniApi.scala)\n\tat com.databricks.photon.JniExecNode.open(JniExecNode.java:74)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.$anonfun$getResult$1(PhotonExec.scala:905)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.photon.PhotonResultHandler.timeit(PhotonResultHandler.scala:30)\n\tat com.databricks.photon.PhotonResultHandler.timeit$(PhotonResultHandler.scala:28)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.timeit(PhotonExec.scala:898)\n\tat com.databricks.photon.PhotonPreShuffleResultHandler.getResult(PhotonExec.scala:905)\n\tat com.databricks.photon.PhotonBasicEvaluatorFactory$PhotonBasicEvaluator$$anon$1.hasNext(PhotonBasicEvaluatorFactory.scala:233)\n\tat com.databricks.photon.CloseableIterator$$anon$10.hasNext(CloseableIterator.scala:211)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat com.databricks.photon.MetadataOnlyShuffleWriter.write(MetadataOnlyShuffleWriter.scala:50)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:56)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$3(ShuffleMapTask.scala:92)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.$anonfun$runTask$1(ShuffleMapTask.scala:87)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:58)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:39)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:225)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1045)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:111)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1048)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:935)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: Operation failed: \"The specified path does not exist.\", 404, GET, https://dbxinternalproject.dfs.core.windows.net/dbxintenalproccontainer/Bronze_layer/retail_data_external/part-00003-afdf7063-f52b-4135-86f7-9d3939083c31.c000.snappy.parquet?timeout=90&st=2025-06-09T09:33:13Z&sv=2020-02-10&ske=2025-06-09T11:33:13Z&sig=XXXXX&sktid=1c292f40-07c6-4a6a-9106-a006f98c2da1&se=2025-06-09T11:15:37Z&sdd=2&skoid=e5ca8efa-2529-4105XXXXXXXXXXXXXXXXXX&spr=https&sks=b&skt=2025-06-09T09:33:13Z&sp=rl&skv=2025-01-05&sr=d, PathNotFound, , \"The specified path does not exist. RequestId:6dd9ea1a-d01f-001a-2a28-d9fba0000000 Time:2025-06-09T10:21:51.6995264Z\"\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readRemote(AbfsInputStream.java:684)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readInternal(AbfsInputStream.java:633)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.readOneBlock(AbfsInputStream.java:423)\n\tat shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.AbfsInputStream.read(AbfsInputStream.java:360)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.$anonfun$read$3(LokiABFS.scala:219)\n\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.withExceptionRewrites(LokiABFS.scala:209)\n\tat com.databricks.common.filesystem.LokiAbfsInputStream.read(LokiABFS.scala:219)\n\tat java.io.DataInputStream.read(DataInputStream.java:149)\n\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:107)\n\tat com.databricks.sql.io.PendingFutures.lambda$submit$0(PendingFutures.java:71)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\t... 3 more\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dbx_internalproc.bronze_layer.Batch_tracking_tbl\n",
    "    USING DELTA\n",
    "    LOCATION '{delta_path}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f0a4ec-57b4-45cc-a4cd-ee2ea23499f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+--------------+\n|Batch_id  |insert_timestamp       |Processed_flag|\n+----------+-----------------------+--------------+\n|6122044a-f|2025-06-09 10:32:42.38 |Y             |\n|f9aa20e4-f|2025-06-09 10:48:54.551|Y             |\n|6122044a-f|2025-06-09 10:32:42.38 |N             |\n|4274eabe-1|2025-06-09 11:00:18.119|Y             |\n|f9aa20e4-f|2025-06-09 10:48:54.551|N             |\n|6122044a-f|2025-06-09 10:32:42.38 |N             |\n+----------+-----------------------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "#update bachtracking table\n",
    "Batch_tracking_tbl_main = spark.table(\"dbx_internalproc.bronze_layer.Batch_tracking_tbl\")   \n",
    "Batch_tracking_tbl_main.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5270295249332887,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_Layer",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}